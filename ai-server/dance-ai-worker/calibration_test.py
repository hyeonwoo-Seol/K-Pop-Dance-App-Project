# >> calibration_test.py
# >> ì „ë¬¸ê°€ ì˜ìƒ 2ê°œë¥¼ ë¹„êµí•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì˜ 'í—ˆìš© ì˜¤ì°¨(Baseline Error)'ë¥¼ ì¸¡ì •í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
# >> Source ì˜ìƒì€ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ê³ , User ì˜ìƒì€ [ìˆ˜ë™ ì„ íƒ ëª¨ë“œ]ë¥¼ í†µí•´ íŠ¹ì • ì¸ë¬¼ì„ ì¶”ì í•©ë‹ˆë‹¤.
# >> [ìˆ˜ì •] ID Switching ë°©ì§€: IDê°€ ë°”ë€Œë”ë¼ë„ ì§ì „ ìœ„ì¹˜(Last Position)ì™€ ê°€ì¥ ê°€ê¹Œìš´ ì‚¬ëŒì„ ì°¾ì•„ ì¶”ì ì„ ì´ì–´ê°€ëŠ” ë¡œì§ ì¶”ê°€.

import os
import json
import cv2
import numpy as np
import shutil
from pose_estimation import PoseEstimator
from scoring import Scoring

# >> [í™•ì¥] ìˆ˜ë™ ì„ íƒ ê¸°ëŠ¥ì„ ê°–ì¶˜ ì¶”ì ê¸°
class ManualPoseEstimator(PoseEstimator):
    
    # >> 1ë‹¨ê³„: ì‚¬ëŒì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ í”„ë ˆì„ì„ ë„˜ê¸°ë©° ì¶”ì í•  IDë¥¼ ì„ íƒë°›ëŠ” í•¨ìˆ˜
    def select_target_id(self, video_path):
        if not os.path.exists(video_path):
            print(f"[Error] ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None

        cap = cv2.VideoCapture(video_path)
        
        print("\n[ID ì„ íƒ] ì‚¬ëŒì„ ì°¾ê¸° ìœ„í•´ ì´ˆë°˜ ì˜ìƒì„ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...")
        
        found_valid_frame = False
        display_frame = None
        detected_ids = []
        
        # ìµœëŒ€ 150í”„ë ˆì„(ì•½ 5ì´ˆ)ê¹Œì§€ íƒìƒ‰
        max_search_frames = 150
        frame_idx = 0

        while frame_idx < max_search_frames:
            ret, frame = cap.read()
            if not ret: break

            # YOLO ì¶”ë¡  (ë‹¨ì¼ í”„ë ˆì„)
            results = self.model.track(source=frame, persist=True, verbose=False, device=self.device)
            result = results[0]

            if result.boxes and result.boxes.id is not None:
                track_ids = result.boxes.id.int().cpu().tolist()
                boxes = result.boxes.xyxy.cpu().numpy()
                detected_ids = track_ids
                
                display_frame = frame.copy()
                
                # ë°•ìŠ¤ì™€ ID ê·¸ë¦¬ê¸°
                for idx, track_id in enumerate(track_ids):
                    x1, y1, x2, y2 = map(int, boxes[idx])
                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    
                    label = f"ID: {track_id}"
                    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
                    cv2.rectangle(display_frame, (x1, y1 - 30), (x1 + w, y1), (0, 255, 0), -1)
                    cv2.putText(display_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
                
                found_valid_frame = True
                print(f" >> {frame_idx}ë²ˆì§¸ í”„ë ˆì„ì—ì„œ ì‚¬ëŒ ê°ì§€ ì„±ê³µ!")
                break
            
            frame_idx += 1
        
        cap.release()

        if not found_valid_frame:
            print(f"[Warning] ì´ˆë°˜ {max_search_frames}í”„ë ˆì„ ë™ì•ˆ ì‚¬ëŒì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        preview_path = "id_selection_preview.jpg"
        cv2.imwrite(preview_path, display_frame)
        print(f"[ì•ˆë‚´] ì„ íƒ í™”ë©´ì´ íŒì—…ë©ë‹ˆë‹¤. (ì•ˆ ë³´ì´ë©´ '{preview_path}' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”)")

        try:
            cv2.imshow("Select Target Person", display_frame)
            cv2.waitKey(100) 
        except Exception:
            pass 

        print(f"\nê°ì§€ëœ ID ëª©ë¡: {detected_ids}")
        while True:
            try:
                selection = input(">> ì¶”ì í•  ì‚¬ëŒì˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
                selected_id = int(selection)
                if selected_id in detected_ids:
                    cv2.destroyAllWindows()
                    return selected_id
                else:
                    print(f"ì˜¤ë¥˜: ID {selected_id}ëŠ” ê°ì§€ëœ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("ì˜¤ë¥˜: ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # >> 2ë‹¨ê³„: [í•µì‹¬ ìˆ˜ì •] ID ê¸°ë°˜ ì¶”ì  + ìœ„ì¹˜ ê¸°ë°˜ ë³´ì •(Re-Identification)
    def process_video_specific_id(self, video_path, output_dir, initial_target_id):
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_json_path = os.path.join(output_dir, f"{video_name}_ID_{initial_target_id}.json")
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration_sec = total_frames / fps if fps > 0 else 0
        
        print(f"[Test AI] ì¶”ì  ë¶„ì„ ì‹œì‘: {video_name} (Initial ID: {initial_target_id})")

        json_data = {
            "metadata": {
                "version": "1.2_manual_tracking",
                "model": "yolo11l-pose-manual",
                "video_width": width, "video_height": height,
                "total_frames": total_frames, "fps": fps, "duration_sec": duration_sec
            },
            "summary": {"total_score": 0}, 
            "frames": []
        }
        
        # >> ì¶”ì  ìƒíƒœ ë³€ìˆ˜
        current_target_id = initial_target_id
        last_center_x = None
        last_center_y = None
        
        frame_idx = 0
        valid_frames_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret: break
            
            # YOLO ì¶”ë¡ 
            results = self.model.track(source=frame, persist=True, verbose=False, device=self.device)
            result = results[0]
            
            frame_info = {
                "frame_index": frame_idx,
                "timestamp": float(f"{frame_idx/fps:.4f}") if fps > 0 else 0,
                "is_valid": False,
                "keypoints": []
            }
            
            best_match_idx = -1
            found_by_id = False
            
            if result.boxes and result.boxes.id is not None:
                track_ids = result.boxes.id.int().cpu().tolist()
                boxes_xywh = result.boxes.xywh.cpu().numpy() # x, y, w, h
                
                # 1. IDë¡œ ë¨¼ì € ì°¾ê¸° (ê°€ì¥ ì •í™•í•¨)
                if current_target_id in track_ids:
                    best_match_idx = track_ids.index(current_target_id)
                    found_by_id = True
                
                # 2. IDë¡œ ëª» ì°¾ì•˜ì§€ë§Œ, ì§ì „ ìœ„ì¹˜ê°€ ìˆë‹¤ë©´ ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (ID Switching ëŒ€ì‘)
                elif last_center_x is not None:
                    min_dist = float('inf')
                    
                    for i, t_id in enumerate(track_ids):
                        cx, cy = boxes_xywh[i][0], boxes_xywh[i][1]
                        dist = ((cx - last_center_x)**2 + (cy - last_center_y)**2)**0.5
                        
                        # í™”ë©´ ë„ˆë¹„ì˜ 15% ì´ë‚´ ê±°ë¦¬ì—¬ì•¼ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼ (ê¸‰ê²©í•œ ì´ë™ ì œì™¸)
                        if dist < (width * 0.15):
                            if dist < min_dist:
                                min_dist = dist
                                best_match_idx = i
                                # ì—¬ê¸°ì„œ IDê°€ ë°”ë€Œì—ˆë‹¤ë©´ ì—…ë°ì´íŠ¸ (ì¤‘ìš”!)
                                current_target_id = t_id 
                    
                    if best_match_idx != -1:
                        print(f"   [ID Switch] ID ë³€ê²½ë¨: {initial_target_id} -> {current_target_id} (Frame {frame_idx})")

                # >> ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
                if best_match_idx != -1:
                    keypoints_raw = result.keypoints.data[best_match_idx].cpu().numpy()
                    
                    # í˜„ì¬ ìœ„ì¹˜ ê°±ì‹  (ë‹¤ìŒ í”„ë ˆì„ì„ ìœ„í•´)
                    last_center_x = boxes_xywh[best_match_idx][0]
                    last_center_y = boxes_xywh[best_match_idx][1]
                    
                    # ì •ê·œí™” ë¡œì§ (max_dim ê¸°ì¤€)
                    normalized_kp = []
                    max_dim = max(width, height)
                    
                    for kp in keypoints_raw:
                        x, y, conf = kp
                        norm_x = x / max_dim if max_dim > 0 else 0
                        norm_y = y / max_dim if max_dim > 0 else 0
                        normalized_kp.append([float(f"{norm_x:.5f}"), float(f"{norm_y:.5f}"), float(f"{conf:.4f}")])
                    
                    # Neck ì¶”ê°€
                    l_sh, r_sh = normalized_kp[5], normalized_kp[6]
                    if l_sh[2] > 0 and r_sh[2] > 0:
                        nx, ny = (l_sh[0]+r_sh[0])/2, (l_sh[1]+r_sh[1])/2
                        nc = (l_sh[2]+r_sh[2])/2
                    else:
                        nx, ny, nc = 0, 0, 0
                    normalized_kp.append([float(f"{nx:.5f}"), float(f"{ny:.5f}"), float(f"{nc:.4f}")])
                    
                    frame_info["is_valid"] = True
                    frame_info["keypoints"] = normalized_kp
                    valid_frames_count += 1
            
            # ë§Œì•½ ì‚¬ëŒì„ ì•„ì˜ˆ ë†“ì³¤ë‹¤ë©´? last_center ìœ ì§€ (ì ì‹œ ê°€ë ¤ì¡Œì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ)
            # ë‹¨, ë„ˆë¬´ ì˜¤ë˜ ë†“ì¹˜ë©´(ì˜ˆ: 30í”„ë ˆì„) last_center ì´ˆê¸°í™” ê³ ë ¤ ê°€ëŠ¥ (ì—¬ê¸°ì„  ìœ ì§€)

            json_data["frames"].append(frame_info)
            
            if frame_idx % 100 == 0:
                status = f"ID {current_target_id}" if frame_info["is_valid"] else "Lost"
                print(f"   >> Processing frame {frame_idx}/{total_frames}... ({status})")
            frame_idx += 1
            
        cap.release()
        
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=None)
            
        print(f"[Test AI] ë¶„ì„ ì™„ë£Œ: {output_json_path} (Valid Frames: {valid_frames_count}/{total_frames})")
        return output_json_path

# >> ë“±ê¸‰ ê³„ì‚° (ê¸°ì¡´ ìœ ì§€)
def calculate_grade(score, visibility_ratio=1.0):
    if score >= 90: grade = "S"
    elif score >= 80: grade = "A"
    elif score >= 70: grade = "B"
    else: grade = "C"
    if visibility_ratio < 0.7:
        grades_order = ["S", "A", "B", "C"]
        try:
            current_idx = grades_order.index(grade)
            new_idx = min(current_idx + 1, len(grades_order) - 1)
            grade = grades_order[new_idx]
        except ValueError: pass
    return grade

def run_calibration_test():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    sample_dir = os.path.join(base_dir, 'sampleMP4')
    output_dir = os.path.join(base_dir, 'calibration_result')
    
    source_video_path = os.path.join(sample_dir, 'Cut_AfterLike_source.mp4')
    user_video_path = os.path.join(sample_dir, 'Cut_AfterLike_user.mp4')
    
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    print("==================================================")
    print("   [ ì¡¸ì—…ì‘í’ˆ ] ì „ë¬¸ê°€ vs ì „ë¬¸ê°€ ì˜¤ì°¨ ì¸¡ì • (ID Re-Id ê¸°ëŠ¥ í¬í•¨)")
    print("==================================================")

    try:
        manual_estimator = ManualPoseEstimator(model_path='yolo11l-pose.pt')
        scorer = Scoring()
    except Exception as e:
        print(f"[Error] ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # 1. Source ë¶„ì„ (ìë™)
    print("\n[1ë‹¨ê³„] Source(ê¸°ì¤€) ì˜ìƒ ë¶„ì„")
    source_json_path = manual_estimator.process_video(source_video_path, output_dir)
    final_source_path = os.path.join(output_dir, "source_expert.json")
    if os.path.exists(source_json_path):
        os.rename(source_json_path, final_source_path)

    # 2. User ë¶„ì„ (ìˆ˜ë™ ì„ íƒ + ìë™ ì¶”ì  ë³´ì •)
    print("\n[2ë‹¨ê³„] User(ë¹„êµ) ì˜ìƒ ë¶„ì„")
    selected_id = manual_estimator.select_target_id(user_video_path)
    
    if selected_id is not None:
        user_json_path = manual_estimator.process_video_specific_id(user_video_path, output_dir, selected_id)
        
        # 3. ë¹„êµ
        print("\n[3ë‹¨ê³„] ìµœì¢… ì ìˆ˜ ì‚°ì¶œ")
        try:
            result = scorer.compare(user_json_path, final_source_path)
            if result:
                grade = calculate_grade(result['total_score'], result.get('visibility_ratio', 1.0))
                print("\n==================================================")
                print("   [ ğŸ† í…ŒìŠ¤íŠ¸ ê²°ê³¼ ]")
                print("==================================================")
                print(f"1. ì¢…í•© ì ìˆ˜ (Total Score): {result['total_score']}ì ")
                print(f"2. ë“±ê¸‰ (Grade): {grade}") 
                print(f"3. ì·¨ì•½ ë¶€ìœ„: {result['worst_part']}")
                print(f"4. ìœ íš¨ í”„ë ˆì„ ë¹„ìœ¨: {result.get('visibility_ratio', 0.0):.2f}")
                
                print(f"\n[ê²°ë¡ ]")
                if result['total_score'] >= 85:
                    print(f"âœ… ì„±ê³µ! ì¶”ì ê³¼ ì‹±í¬, ì ìˆ˜ ì‚°ì¶œì´ ëª¨ë‘ ì •ìƒì…ë‹ˆë‹¤.")
                else:
                    print(f"âš ï¸ ì ìˆ˜ê°€ {result['total_score']}ì ì…ë‹ˆë‹¤. ì˜¤ë²„ë ˆì´ ì˜ìƒì„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")
        except Exception as e:
            print(f"ë¹„êµ ì‹¤íŒ¨: {e}")
    else:
        print("[ì·¨ì†Œ] ID ì„ íƒ ì‹¤íŒ¨")

if __name__ == "__main__":
    run_calibration_test()
